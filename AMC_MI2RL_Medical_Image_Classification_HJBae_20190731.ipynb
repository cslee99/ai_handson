{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AMC_MI2RL_Medical_Image_Classification_HJBae_20190731.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6GWbSx9-KNd",
        "colab_type": "text"
      },
      "source": [
        "# MI2RL Deep Learning Hands-on (2): Medical Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3SbO4_C-mxh",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://adeshpande3.github.io/assets/Cover.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WaZl5UEFoiY",
        "colab_type": "text"
      },
      "source": [
        "# 0. 시작에 앞서... \n",
        "\n",
        "Menu -> Runtime -> Change runtime type "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYy0UfRvFtyJ",
        "colab_type": "text"
      },
      "source": [
        "![gpu setting](https://raw.githubusercontent.com/mi2rl/datasets/master/gpu.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROKfeR4sB_38",
        "colab_type": "text"
      },
      "source": [
        "# 1. 데이터 준비: MedNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhbWZItrGoeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 다운로드\n",
        "!wget https://raw.githubusercontent.com/mi2rl/datasets/master/mednist.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm07t8jnHFcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 압축 풀기\n",
        "!tar xzf mednist.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMQbFRjVHFfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 패키지 불러오기\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H4_fQZNHFh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataDir = 'resized'               # 데이터 위치\n",
        "classNames = os.listdir(dataDir)  # 각 클래스의 이름들\n",
        "numClass = len(classNames)        # Number of classes = number of subdirectories\n",
        "imageFiles = [[os.path.join(dataDir,classNames[i],x) for x in os.listdir(os.path.join(dataDir,classNames[i]))]\n",
        "              for i in range(numClass)]                     # 각 클래스 별 파일 이름들\n",
        "numEach = [len(imageFiles[i]) for i in range(numClass)]     # 각 클래스 별 파일 갯수\n",
        "imageFilesList = []               # 모든 파일이름\n",
        "imageClass = []                   # 각각의 파일들에 대한 클래스 \n",
        "\n",
        "for i in range(numClass):\n",
        "    imageFilesList.extend(imageFiles[i])\n",
        "    imageClass.extend([i]*numEach[i])\n",
        "    \n",
        "numTotal = len(imageClass)        # 전체 파일 갯수\n",
        "imageWidth, imageHeight = Image.open(imageFilesList[0]).size         # 각 영상의 사이즈(width, height)\n",
        "\n",
        "print(\"There are\",numTotal,\"images in\",numClass,\"distinct categories\")\n",
        "print(\"Label names:\",classNames)\n",
        "print(\"Label counts:\",numEach)\n",
        "print(\"Image dimensions:\",imageWidth,\"x\",imageHeight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrxbvSerHFki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 전체 이미지 중 9개를 랜덤으로 골라 3x3으로 레이블과 함께 그리기\n",
        "# -- 여러번 실행하며 이미지들을 살펴보세요 --\n",
        "\n",
        "plt.subplots(3,3,figsize=(8,8))\n",
        "for i,k in enumerate(np.random.randint(numTotal, size=9)): \n",
        "    im = Image.open(imageFilesList[k])                      \n",
        "    arr = np.array(im)\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.xlabel(classNames[imageClass[k]])\n",
        "    plt.imshow(arr,cmap='gray',vmin=0,vmax=255)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXOTdcrs4QVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 이미지 리스트 살펴보기\n",
        "imageFilesList[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc7wMIt4FiB_",
        "colab_type": "text"
      },
      "source": [
        "# 2. VGG16를 이용한 분류 실습 (w/ ImageNet pre-trained weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9pA19KsFZHG",
        "colab_type": "text"
      },
      "source": [
        "![VGG16 네트워크 구조](https://www.cs.toronto.edu/~frossard/post/vgg16/vgg16.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42X_EccV9fgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQydjH4l9tAG",
        "colab_type": "text"
      },
      "source": [
        "# 2.1. [Quiz] 순서가 섞인 layer들을 VGG16 구성에 맞게 배치해보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQCGxNBH9G09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = Input(shape=(224, 224, 3,), name=\"VGGInput\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6HvoFRi-T5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation=relu)(inputs)\n",
        "x = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation=relu)(x)\n",
        "x = MaxPool2D(padding='same')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMjc4HcX-UCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation=relu)(x)\n",
        "x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation=relu)(x)\n",
        "x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation=relu)(x)\n",
        "x = MaxPool2D(padding='same')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkjP3szO-T8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation=relu)(x)\n",
        "x = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation=relu)(x)\n",
        "x = MaxPool2D(padding='same')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHfkHQba-T_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation=relu)(x)\n",
        "x = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation=relu)(x)\n",
        "x = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation=relu)(x)\n",
        "x = MaxPool2D(padding='same')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnFNuXG6-ePI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Conv2D(filters=4096, kernel_size=(7,7), padding='valid', activation=relu)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(4096, activation=relu)(x)\n",
        "pred = Dense(1000, activation=softmax)(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIutE6mo-Mez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation=relu)(x)\n",
        "x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation=relu)(x)\n",
        "x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation=relu)(x)\n",
        "x = MaxPool2D(padding='same')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO5AiwPf-MiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(inputs=inputs, outputs=pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhErtRaK9pfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ikL97iT9ph3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = Adam(learning_rate=1e-03)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H00490Rz9pk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8grXT5RWAR7W",
        "colab_type": "text"
      },
      "source": [
        "# 2.2. VGG16 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_KUJrPaAiSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import vgg16\n",
        "\n",
        "# VGG16 모델 불러오기\n",
        "model = vgg16.VGG16()\n",
        "\n",
        "# 모델의 모양을 보여준다.\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_UFP5cCJKuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model 구성도 plot\n",
        "from IPython.display import Image\n",
        "\n",
        "keras.utils.plot_model(model, to_file='vgg16.png', show_shapes=True, show_layer_names=True)\n",
        "Image(filename='vgg16.png')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oe8WYVBEh4K",
        "colab_type": "text"
      },
      "source": [
        "**VGG16**\n",
        "\n",
        "`keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)`\n",
        "\n",
        "\n",
        "VGG16 model, with weights pre-trained on ImageNet.\n",
        "\n",
        "This model can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels).\n",
        "\n",
        "The default input size for this model is 224x224.\n",
        "\n",
        "**Arguments**\n",
        "\n",
        "\n",
        "\n",
        "*   include_top: whether to include the 3 fully-connected layers at the top of the network.\n",
        "*   weights: one of None (random initialization) or 'imagenet' (pre-training on ImageNet).\n",
        "*   input_tensor: optional Keras tensor (i.e. output of layers.Input()) to use as image input for the model.\n",
        "*   input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value.\n",
        "*   pooling: Optional pooling mode for feature extraction when include_top is False.\n",
        "*   classes: optional number of classes to classify images into, only to be specified if include_top is  True, and if no weights argument is specified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmJOZlNSC30k",
        "colab_type": "text"
      },
      "source": [
        "# keras 에서 제공되는 모델들 참고: https://keras.io/applications/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hpF46233PXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VGG16 모델을 이용해 prediction 하는 함수\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from IPython.display import display # 이미지 출력 함수\n",
        "\n",
        "def predict_vgg16(model, filename) :\n",
        "    # 이미지 파일을 읽고 화면에 표시\n",
        "    image = load_img(filename)\n",
        "    display(image)\n",
        "\n",
        "    # 모델 사이즈로 이미지 파일을 읽기\n",
        "    image = load_img(filename, target_size=(224, 224))\n",
        "\n",
        "    # 이미지 데이터를 numpy로 변환\n",
        "    image = img_to_array(image)\n",
        "\n",
        "    # vgg16.preprocess_input()을 호출하기 위해 차원을 조정\n",
        "    # 보통 모델을 여러 이미지를 한번에 호출. \n",
        "    # 맨 앞의 1 : 이미지 갯수가 1개라는 것.\n",
        "    # 두번째 224 : 가로\n",
        "    # 세번째 224 : 세로\n",
        "    # 네번째 3 : R, G, B 3개\n",
        "    image = image.reshape((1, 224, 224, 3))\n",
        "\n",
        "    # VGG16 모델 호출을 위해 데이터 전처리.\n",
        "    # -255 ~ 255 사이 값으로 정규화한다.\n",
        "    # 그리고 RGB를 BGR순으로 바꾼다.\n",
        "    image = vgg16.preprocess_input(image)\n",
        "\n",
        "\n",
        "    # 이미지를 모델에 적용\n",
        "    yhat = model.predict(image)\n",
        "\n",
        "    # 모델 적용된 결과를 파싱\n",
        "    label = vgg16.decode_predictions(yhat)\n",
        "\n",
        "    # 가장 확률이 높은 결과를 획득\n",
        "    label = label[0][0]\n",
        "\n",
        "    # 라벨과 라벨을 예측한 확률을 출력\n",
        "    print('%s (%.2f%%)' % (label[1], label[2]*100))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owkpMH9i3Ph8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = imageFilesList[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXZ7oCZ13Pkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for file in files:\n",
        "  predict_vgg16(model, file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWjJxcdDF_Bi",
        "colab_type": "text"
      },
      "source": [
        "# 2.3. Dataset 나누기: Train / Validation / Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXKqQ0ig61ve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validFrac = 0.2   # Define the fraction of images to move to validation dataset\n",
        "testFrac = 0.2    # Define the fraction of images to move to test dataset\n",
        "validList = []\n",
        "testList = []\n",
        "trainList = []\n",
        "\n",
        "for i in range(numTotal):\n",
        "    rann = np.random.random() # Randomly reassign images\n",
        "    if rann < validFrac:\n",
        "        validList.append(i)\n",
        "    elif rann < testFrac + validFrac:\n",
        "        testList.append(i)\n",
        "    else:\n",
        "        trainList.append(i)\n",
        "        \n",
        "nTrain = len(trainList)  # Count the number in each set\n",
        "nValid = len(validList)\n",
        "nTest = len(testList)\n",
        "print(\"Training images =\",nTrain,\"Validation =\",nValid,\"Testing =\",nTest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QWKb_Aw69us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ./train\n",
        "!mkdir ./valid\n",
        "!mkdir ./test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQh-O9Ai9IUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "for i in tqdm(range(len(trainList))):\n",
        "  root, clas, src = imageFilesList[trainList[i]].split('/')\n",
        "  dest = os.path.join('./train',clas,src)\n",
        "  if not os.path.exists(os.path.join('./train',clas)):\n",
        "    os.mkdir(os.path.join('./train',clas))\n",
        "  shutil.copy(imageFilesList[trainList[i]], dest)\n",
        "  \n",
        "for i in tqdm(range(len(validList))):\n",
        "  root, clas, src = imageFilesList[validList[i]].split('/')\n",
        "  dest = os.path.join('./valid',clas,src)\n",
        "  if not os.path.exists(os.path.join('./valid',clas)):\n",
        "    os.mkdir(os.path.join('./valid',clas))\n",
        "  shutil.copy(imageFilesList[validList[i]], dest)\n",
        "  \n",
        "    \n",
        "for i in tqdm(range(len(testList))):\n",
        "  root, clas, src = imageFilesList[testList[i]].split('/')\n",
        "  dest = os.path.join('./test',clas,src)\n",
        "  if not os.path.exists(os.path.join('./test',clas)):\n",
        "    os.mkdir(os.path.join('./test',clas))\n",
        "  shutil.copy(imageFilesList[testList[i]], dest)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OALwHXmrGPIO",
        "colab_type": "text"
      },
      "source": [
        "# 2.4. Image Data Generator 정의 (+Data Augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzlyOZr-4xqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_dir = './train'\n",
        "validation_dir = './valid'\n",
        "batch_size = 32\n",
        "image_size = 224\n",
        "\n",
        "# 학습에 사용될 이미지 데이터 생성기\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rotation_range=180, # 회전 최대 20도\n",
        "      width_shift_range=0.2, # 좌우 이동\n",
        "      height_shift_range=0.2, # 상하 이동\n",
        "      horizontal_flip=True, # 좌우 반전\n",
        "      vertical_flip=True, # 상하 반전\n",
        "      )\n",
        " \n",
        "# 검증에 사용될 이미지 데이터 생성기\n",
        "validation_datagen = ImageDataGenerator()\n",
        " \n",
        "\n",
        "# 학습에 사용될 데이터 생성기  \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True)\n",
        "\n",
        "# 검증에 사용될 데이터 생성기\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "\n",
        "class_num=len(train_generator.class_indices)\n",
        "\n",
        "custom_labels = list(validation_generator.class_indices.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59xxKdQ24xtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from keras.models import Model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.applications import VGG16\n",
        "import keras.backend as K\n",
        "\n",
        "K.clear_session() # 새로운 세션으로 시작"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Omc81vxOlww",
        "colab_type": "text"
      },
      "source": [
        "# 2.5. VGG16 Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZJLsoWNHIaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델 불러오기\n",
        "conv_layers = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
        "conv_layers.summary()\n",
        "\n",
        "# Convolution Layer를 학습되지 않도록 고정 \n",
        "for layer in conv_layers.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvV31g1IGmu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 새로운 모델 생성하기\n",
        "model = models.Sequential()\n",
        "\n",
        "# VGG16모델의 Convolution Layer를 추가\n",
        "model.add(conv_layers)\n",
        " \n",
        "# 모델의 Fully Connected 부분을 재구성\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(class_num, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpvmrwYPGm8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 새로운 모델 요약\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHHjLfRGGm_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 새로운 모델 저장\n",
        "vgg16_model_path = 'new_trained_from_vgg16.h5'\n",
        "\n",
        "model.save(vgg16_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0XDfcreHdsY",
        "colab_type": "text"
      },
      "source": [
        "# 2.6. 새로운 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wEl365N4xwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# 모델 로딩\n",
        "model = load_model(vgg16_model_path)\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100 ,\n",
        "      epochs=10,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "      verbose=1)\n",
        " \n",
        "# 모델 저장\n",
        "model.save(vgg16_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BicDThpJMjYe",
        "colab_type": "text"
      },
      "source": [
        "# 2.7. 학습 결과 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APTOYXtQKmHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "valacc = history.history['val_acc']\n",
        "valloss = history.history['val_loss']\n",
        "\n",
        "\n",
        "epochs = range(len(acc))\n",
        " \n",
        "plt.plot(epochs, acc, 'b', label='accuracy')\n",
        "plt.plot(epochs, loss, 'r', label='loss')\n",
        "plt.plot(epochs, valacc, 'b--', label='val_accuracy')\n",
        "plt.plot(epochs, valloss, 'r--', label='val_loss')\n",
        "plt.title('accuracy and loss')\n",
        "plt.legend()\n",
        "  \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8LkTxYwMhVr",
        "colab_type": "text"
      },
      "source": [
        "# 2.8. 학습된 모델을 이용해 Test 데이터에 대한 Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6_aDzzpVFou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dir = './test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifa-qfYCU25r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_datagen = ImageDataGenerator()\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=1,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "\n",
        "model.evaluate_generator(test_generator, steps=test_generator.samples, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBweoAb7VaOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmV0OYqXNa9A",
        "colab_type": "text"
      },
      "source": [
        "# 3. VGG16를 이용한 분류 실습 (from Scratch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T46Avh8YNsOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델 불러오기\n",
        "vgg_model = VGG16(weights=None, include_top=False, input_shape=(image_size, image_size, 3))\n",
        "vgg_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67k4voNSwEOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last = vgg_model.output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTvTMNkUNsLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VGG16모델에 Fully Connected부분을 재구성해서 추가\n",
        "x = Flatten()(last)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "pred = Dense(class_num, activation='softmax')(x)\n",
        "\n",
        "model = Model(vgg_model.input, pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuYjR4LVNsJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 새로운 모델 요약\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OObH3RkkNsGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 새로운 모델 저장\n",
        "vgg16_model_path = 'new_trained_from_vgg16_scratch.h5'\n",
        "\n",
        "model.save(vgg16_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSh_mDgUNsDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델 로딩\n",
        "model = load_model(vgg16_model_path)\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100 ,\n",
        "      epochs=5,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "      verbose=1)\n",
        " \n",
        "# 모델 저장\n",
        "model.save(vgg16_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNhECKJ0NsBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "valacc = history.history['val_acc']\n",
        "valloss = history.history['val_loss']\n",
        "\n",
        "\n",
        "epochs = range(len(acc))\n",
        " \n",
        "plt.plot(epochs, acc, 'b', label='accuracy')\n",
        "plt.plot(epochs, loss, 'r', label='loss')\n",
        "plt.plot(epochs, valacc, 'b--', label='val_accuracy')\n",
        "plt.plot(epochs, valloss, 'r--', label='val_loss')\n",
        "plt.title('accuracy and loss')\n",
        "plt.legend()\n",
        "  \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP-GYJBsNr-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dir = './test'\n",
        "\n",
        "test_datagen = ImageDataGenerator()\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=1,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "# For evaluation purposes\n",
        "#model.evaluate_generator(test_generator, steps=test_generator.samples, verbose=1)\n",
        "\n",
        "# For prediction purposes\n",
        "y_pred = model.predict_generator(test_generator, steps=test_generator.samples, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl3_xptWXSdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYACbR5kZNg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred0 = np.argmax(y_pred, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxe0VuH0ZUWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPt7DlIsUw6U",
        "colab_type": "text"
      },
      "source": [
        "# 4. Confusion Matrix Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-P5meT9WN9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj370N6rU2jN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y_test labeling\n",
        "y_test = test_generator.labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhqlquNIU2gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate confusion matrix for the predicted dataset\n",
        "cm = confusion_matrix(y_test, y_pred0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR90a3Fiaibu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make a dataframe using cm array\n",
        "df_cm = pd.DataFrame(cm, index = [i for i in classNames], columns = [i for i in classNames])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQzfqDSNaiZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot confusion matrix\n",
        "plt.figure(figsize = (10, 7))\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h5U1n_EaiW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classification report generation: precision, recall, f1-score. \n",
        "print(classification_report(y_test, y_pred0, target_names=classNames))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBlHqWhCM9gL",
        "colab_type": "text"
      },
      "source": [
        "# 5. VGG16 + GradCAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8aLprq3y4mh",
        "colab_type": "text"
      },
      "source": [
        "![gradCAM](https://camo.githubusercontent.com/450498bd998fd99d51b647d2b6c8631e94585522/687474703a2f2f692e696d6775722e636f6d2f4a614762645a352e706e67)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb96qSvky9jM",
        "colab_type": "text"
      },
      "source": [
        "**Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization**\n",
        "Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, Dhruv Batra\n",
        "https://arxiv.org/abs/1610.02391"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sLMeFLDzVAD",
        "colab_type": "text"
      },
      "source": [
        "**Example: 'Boxer'**\n",
        "\n",
        "![alt text](https://github.com/PowerOfCreation/keras-grad-cam/raw/master/examples/cat_dog.png) \n",
        "![alt text](https://github.com/PowerOfCreation/keras-grad-cam/raw/master/examples/cat_dog_242_gradcam.jpg)\n",
        "![alt text](https://github.com/PowerOfCreation/keras-grad-cam/raw/master/examples/cat_dog_242_guided_gradcam.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHuXdrjxeQNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GradCAM code from https://github.com/PowerOfCreation/keras-grad-cam/blob/master/grad-cam.py\n",
        "\n",
        "from keras.applications.vgg16 import (\n",
        "    VGG16, preprocess_input, decode_predictions)\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "from keras.layers.core import Lambda\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from tensorflow.python.framework import ops\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import sys\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "def target_category_loss(x, category_index, nb_classes):\n",
        "    return tf.multiply(x, K.one_hot([category_index], nb_classes))\n",
        "\n",
        "def target_category_loss_output_shape(input_shape):\n",
        "    return input_shape\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "def load_images(path):\n",
        "    img_path = sys.argv[1]\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    return x\n",
        "\n",
        "def register_gradient():\n",
        "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
        "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
        "        def _GuidedBackProp(op, grad):\n",
        "            dtype = op.inputs[0].dtype\n",
        "            return grad * tf.cast(grad > 0., dtype) * \\\n",
        "                tf.cast(op.inputs[0] > 0., dtype)\n",
        "\n",
        "def compile_saliency_function(model, activation_layer='block5_conv3'):\n",
        "    input_img = model.input\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
        "    layer_output = layer_dict[activation_layer].output\n",
        "    max_output = K.max(layer_output, axis=3)\n",
        "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
        "    return K.function([input_img, K.learning_phase()], [saliency])\n",
        "\n",
        "def modify_backprop(model, name):\n",
        "    g = tf.get_default_graph()\n",
        "    with g.gradient_override_map({'Relu': name}):\n",
        "\n",
        "        # get layers that have an activation\n",
        "        layer_dict = [layer for layer in model.layers[1:]\n",
        "                      if hasattr(layer, 'activation')]\n",
        "\n",
        "        # replace relu activation\n",
        "        for layer in layer_dict:\n",
        "            if layer.activation == keras.activations.relu:\n",
        "                layer.activation = tf.nn.relu\n",
        "\n",
        "        # re-instanciate a new model\n",
        "        new_model = VGG16(weights='imagenet')\n",
        "    return new_model\n",
        "\n",
        "def deprocess_image(x):\n",
        "    '''\n",
        "    Same normalization as in:\n",
        "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
        "    '''\n",
        "    if np.ndim(x) > 3:\n",
        "        x = np.squeeze(x)\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    if K.image_dim_ordering() == 'th':\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def _compute_gradients(tensor, var_list):\n",
        "    grads = tf.gradients(tensor, var_list)\n",
        "    return [grad if grad is not None else tf.zeros_like(var) for var, grad in zip(var_list, grads)]\n",
        "\n",
        "def grad_cam(input_model, image, category_index):\n",
        "    nb_classes = 6\n",
        "    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\n",
        "    x = Lambda(target_layer, output_shape = target_category_loss_output_shape)(input_model.output)\n",
        "    model = Model(inputs=input_model.input, outputs=x)\n",
        "    loss = K.sum(model.output)\n",
        "    \n",
        "    conv_output =  model.layers[17].output\n",
        "    grads = normalize(_compute_gradients(loss, [conv_output])[0])\n",
        "    gradient_function = K.function([model.input], [conv_output, grads])\n",
        "\n",
        "    output, grads_val = gradient_function([image])\n",
        "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
        "\n",
        "    weights = np.mean(grads_val, axis = (0, 1))\n",
        "    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n",
        "\n",
        "    for i, w in enumerate(weights):\n",
        "        cam += w * output[:, :, i]\n",
        "\n",
        "    cam = cv2.resize(cam, (224, 224))\n",
        "    cam = np.maximum(cam, 0)\n",
        "    heatmap = cam / np.max(cam)\n",
        "\n",
        "    #Return to BGR [0..255] from the preprocessed image\n",
        "    image = image[0, :]\n",
        "    image -= np.min(image)\n",
        "    image = np.minimum(image, 255)\n",
        "\n",
        "    cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
        "    cam = np.float32(cam) + np.float32(image)\n",
        "    cam = 255 * cam / np.max(cam)\n",
        "    return np.uint8(cam), heatmap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdBPXn1sgiiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg16_model_path = 'new_trained_from_vgg16_scratch.h5'\n",
        "model = load_model(vgg16_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA49Cg2RspTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWcEawbDgif0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select a random query image\n",
        "inum = random.randrange(0,len(imageFilesList))\n",
        "qimage = load_img(imageFilesList[inum], target_size=(224, 224))\n",
        "qimage = img_to_array(qimage)\n",
        "qimage = qimage.reshape((1, 224, 224, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLqeOFlneR26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(qimage)\n",
        "predicted_class = np.argmax(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OylOy4mwhGPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grad-CAM Calculation & Visualization\n",
        "cam, heatmap = grad_cam(model, qimage, predicted_class)\n",
        "plt.title(classNames[predicted_class])\n",
        "plt.imshow(cam)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqXRZPbmeRum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "register_gradient()\n",
        "guided_model = modify_backprop(model, 'GuidedBackProp')\n",
        "saliency_fn = compile_saliency_function(guided_model)\n",
        "saliency = saliency_fn([qimage, 0])\n",
        "gradcam = saliency[0] * heatmap[..., np.newaxis]\n",
        "plt.imshow(gradcam[0])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}